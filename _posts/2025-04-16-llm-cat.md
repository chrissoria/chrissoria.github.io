---
title: 'A Tool for Demographers and Social Scientists to Categorize Open-Ended Survey Data: CatLLM'
date: 2025-04-17
permalink: /posts/2025/4/cat-llm-test/
tags:
  - Categorization
  - Survey Data
  - Large Language Models
  - Python Package
---

![CatLLM](/images/wide_cat_llm.png)

Today, we got to beta test a new tool we built for demographers and social scientists who want to make sense of open-ended survey responses.

Our Python package, called **CatLLM**, gives social scientists two main features:

1. **Extract categories** in a set of survey responses.
2. **Sort answers into categories**
   - You can choose if each answer fits into one category or more than one.

## Key Features

- **Very easy to use:** Just enter your list of categories and your survey answers—the package handles the rest.
- **Always gives structured results:** In our testing, the output is always organized and reliable and ready to plug into statistical models.
- **Works with any survey question:** Designed to give the most accurate results using proven methods.
- **Compatible with all OpenAI models:** (We plan to add support for Claude and Meta models in future updates.)
- **Saves money:** Skips over any missing survey answers so you don’t waste money on unnecessary API calls.
- **Automatic saving:** If your system crashes, your work is safe—no lost progress.
- **Category check:** The package double-checks that you entered the categories you wanted.
- **Flexible sorting:** Offers both single-category and multi-category sorting options.

![CatLLM-function](/images/LLM-cat.png)

## Beta Test Results

Jake Derr, a data scientist at UC Berkeley, helped us test CatLLM. We used a set of real job categories (based on official government codes) and ran the tool on 6,399 survey responses. The process took about about 4 hours (236 minutes) and cost around $12. For comparison, Jake manually sorted the same data in about 20 hours. 

The tool’s results matched Jake’s choices most of the time. They disagreed on about 8.8% of the answers, which is similar to the kind of disagreement you’d expect between two people doing the same task. Most of these differences were due to reasonable differences in interpretation, not mistakes. Sometimes, either the model or Jake made a small error.

During testing, we (I) accidentally entered 23 categories instead of 24 (missed a comma!). To prevent this in the future, we added a feature that asks users to double-check their category list.

Overall, CatLLM helped us sort survey responses faster and more efficiently—without needing to hire a third person to double-check everything.

While CatLLM and similar large language model tools can greatly speed up and reduce the cost of annotation—sometimes by a factor of 30 or more compared to traditional human annotation—they are not meant to replace the valuable work of human annotators like Jake. Research shows that while LLMs can outperform crowd workers on some tasks and offer substantial cost savings, there are still cases where human expertise is essential, especially for ambiguous or complex data points. The best results come from combining the speed and efficiency of LLMs with human oversight, ensuring high-quality, reliable annotations.

Next, we’ll review the answers where Jake and the model disagreed. We’ll talk through each case and make a final decision together. Human judgment always comes first: if there’s a disagreement, we discuss it and go with the human’s choice.

![human-model-cooperation](/images/human-llm-feedback.png)

We tested the function llm_extract_multi_class:

```python
jobs_categorized = llm_extract_multi_class(
    survey_question = job_question, 
    survey_input = jobs['Response'], 
    filename = save_location,
    categories = job_categories,
    safety= True,
    to_csv = True)
```

---

## Examples of Disagreements Between the Model and Jake

Here are a few examples where CatLLM and Jake categorized the same job title differently:

1. **"Creative Lead"**  
   - Jake: *Computer and Mathematical Occupations*  
   - Model: *Arts, Design, Entertainment, Sports, and Media Occupations*

2. **"Analyst"**  
   - Jake: *Life, Physical, and Social Science Occupations*  
   - Model: *Business and Financial Occupations*

3. **"Circuit Design"**  
   - Jake: *Architecture and Engineering Occupations*  
   - Model: *Computer and Mathematical Occupations*

4. **"Server"**  
   - Jake: *Food Preparation and Serving Related Occupations*  
   - Model: *Sales and Related Occupations*

5. **"Program Management"**  
   - Jake: *Computer and Mathematical Occupations*  
   - Model: *Management Occupations*

6. **"Workforce Coordinator"**  
   - Jake: *Business and Financial Operations Occupations*  
   - Model: *Management Occupations*

7. **"Vet SEC"**  
   - This one was tricky. "Vet SEC" is actually a security company.  
     - Jake: *Healthcare Practitioners and Technical Occupations* (he thought it was related to veterinary care)  
     - Model: Marked three categories, including the correct one: *Protective Services*. It also selected "Unclear" because it wasn’t sure.

8. **"External vendor management"**  
   - Jake: *Sales and Related Occupations*  
   - Model: *Business and Financial Operations Occupations*

9. **"Order support"**  
   - Jake: *Transportation and Material Moving Occupations*  
   - Model: *Office and Administrative Support Occupations*

10. **"Public Policy Specialist"**  
    - Jake: *Business and Financial Operations Occupations*  
    - Model: *Life, Physical, and Social Science Occupations*

These examples show that even with clear job titles, people and models can interpret them differently. Most disagreements were reasonable and highlight the importance of human review in the final decision process.
